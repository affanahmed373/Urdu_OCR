{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu9BggHEGc933wLp9JJesd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/affanahmed373/Urdu_OCR/blob/main/Urdu_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6KTcnOzP0Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c01b0b5-eafc-49e7-8b74-a285378ad91c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM4IoAGKZGgi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models, Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnph0fTFYTi8"
      },
      "source": [
        "Data Augmentation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSxfE_u7kCOm"
      },
      "source": [
        "def add_gaussian_noise(image):\n",
        "    # image must be scaled in [0, 1]\n",
        "    with tf.name_scope('Add_gaussian_noise'):\n",
        "        noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=(50)/(255), dtype=tf.float32)\n",
        "        noise_img = image + noise\n",
        "        noise_img = tf.clip_by_value(noise_img, 0.0, 1.0)\n",
        "    return noise_img\n",
        "\n",
        "from scipy.signal import convolve2d\n",
        "def GradientMagnitude(image):\n",
        "\n",
        "    # 4. Gradient Magnitude\n",
        "    kernerl1 = np.array([\n",
        "        [-1,-1,-1],\n",
        "        [0,0,0],\n",
        "        [1,1,1]\n",
        "    ])\n",
        "    kernerl2 = np.array([\n",
        "        [-1,0,1],\n",
        "        [-1,0,1],\n",
        "        [-1,0,1]\n",
        "    ])\n",
        "    \n",
        "    # temp1 = convolve2d(image,kernerl1,mode='same')\n",
        "    # temp2 = convolve2d(image,kernerl2,mode='same')\n",
        "    \n",
        "    temp1 = convolve2d(image,kernerl1,mode='same')\n",
        "    temp2 = convolve2d(image,kernerl2,mode='same')\n",
        "    \n",
        "    temp3 = np.sqrt(temp1**2 + temp2**2)\n",
        "    \n",
        "    plt.axis('off')\n",
        "    plt.imshow(temp3,cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    return temp3\n",
        "\n",
        "def GradientDirection(image):\n",
        "\n",
        "    # 5. Gradient Direction\n",
        "    kernerl1 = np.array([\n",
        "        [-1,-1,-1],\n",
        "        [0,0,0],\n",
        "        [1,1,1]\n",
        "    ])\n",
        "    kernerl2 = np.array([\n",
        "        [-1,0,1],\n",
        "        [-1,0,1],\n",
        "        [-1,0,1]\n",
        "    ])\n",
        "    \n",
        "    temp1 = convolve2d(image,kernerl1,mode='same')\n",
        "    temp2 = convolve2d(image,kernerl2,mode='same')\n",
        "    \n",
        "    temp3 = np.arctan(temp1/temp2)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(temp3,cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    return temp3\n",
        "\n",
        "def SobelGradientMagnitude(image):\n",
        "\n",
        "    # 6. Sobel Gradient Magnitude\n",
        "    kernerl1 = np.array([\n",
        "        [1,2,1],\n",
        "        [0,0,0],\n",
        "        [-1,-2,-1]\n",
        "    ])\n",
        "    kernerl2 = np.array([\n",
        "        [1,0,-1],\n",
        "        [2,0,-2],\n",
        "        [1,0,-1]\n",
        "    ])\n",
        "    \n",
        "    temp1 = convolve2d(image,kernerl1,mode='same')\n",
        "    temp2 = convolve2d(image,kernerl2,mode='same')\n",
        "    \n",
        "    temp3 = np.sqrt(temp1**2 + temp2**2)\n",
        "    \n",
        "    # plt.axis('off')\n",
        "    # plt.imshow(temp3,cmap='gray')\n",
        "    # plt.show()\n",
        "    \n",
        "    return temp3\n",
        "\n",
        "def SobelGradientDirection(image):\n",
        "\n",
        "    # 7. Sobel Gradient Direction\n",
        "    kernerl1 = np.array([\n",
        "        [1,2,1],\n",
        "        [0,0,0],\n",
        "        [-1,-2,-1]\n",
        "    ])\n",
        "    kernerl2 = np.array([\n",
        "        [1,0,-1],\n",
        "        [2,0,-2],\n",
        "        [1,0,-1]\n",
        "    ])\n",
        "    \n",
        "    temp1 = convolve2d(image,kernerl1,mode='same')\n",
        "    temp2 = convolve2d(image,kernerl2,mode='same')\n",
        "    \n",
        "    temp3 = np.arctan(temp1/temp2)\n",
        "    \n",
        "    plt.axis('off')\n",
        "    plt.imshow(temp3,cmap='gray')\n",
        "    plt.show()\n",
        "    \n",
        "    return temp3\n",
        "\n",
        "def GuassianBlur(image):\n",
        "\n",
        "    # 8. Guassian Blur\n",
        "    import scipy \n",
        "    \n",
        "    temp = scipy.ndimage.filters.gaussian_filter(\n",
        "        image,\n",
        "        sigma = 10\n",
        "        )\n",
        "    plt.axis('off')\n",
        "    plt.imshow(temp,cmap='gray')\n",
        "    plt.show()\n",
        "    \n",
        "    return temp\n",
        "\n",
        "def Sharpening(image):\n",
        "\n",
        "    # 9. Sharpening\n",
        "    kernerl1 = np.array([\n",
        "        [1,1,1],\n",
        "        [0,0,0],\n",
        "        [-1,-1,-1]\n",
        "    ])\n",
        "    kernerl2 = np.array([\n",
        "        [1,0,-1],\n",
        "        [1,0,-1],\n",
        "        [1,0,-1]\n",
        "    ])\n",
        "    \n",
        "    temp1 = convolve2d(image,kernerl1,mode='same')\n",
        "    temp2 = convolve2d(image,kernerl2,mode='same')\n",
        "    \n",
        "    temp3 = np.sqrt(temp1**2 + temp2**2)\n",
        "    \n",
        "    plt.axis('off')\n",
        "    plt.imshow(temp3 +image ,cmap='gray')\n",
        "    plt.show()\n",
        "    \n",
        "    return temp3+image\n",
        "    \n",
        "def Emboss(image, param):\n",
        "    \n",
        "    # 10. Emboss\n",
        "    kernerl = np.array([\n",
        "        [-1,-1,0],\n",
        "        [-1,0,1],\n",
        "        [0,1,1]\n",
        "    ])\n",
        "    \n",
        "    temp = convolve2d(image,kernerl,mode='same') + param\n",
        "    plt.axis('off')\n",
        "    plt.imshow(temp,cmap='gray')\n",
        "    plt.show()\n",
        "    \n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6A2ug9AZTnm"
      },
      "source": [
        "DATADIR = \"/content/drive/MyDrive/dataset1/ocrdata\"\n",
        "FOLDERS = [\"0\",\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "CATEGORIES = [\"0\",\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "NUM_CLASSES = 10\n",
        "IMG_SIZE = 32\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "jf4msEB8eZfE",
        "outputId": "6eb07dd5-7f67-4eef-af1a-32bd14de9efa"
      },
      "source": [
        "def read_data():\n",
        "    images = []\n",
        "    labels = []\n",
        "    img_paths = []\n",
        "    data = []\n",
        "\n",
        "    for category,folder in zip(CATEGORIES,FOLDERS):  # do B and M \n",
        "        path = os.path.join(DATADIR,folder)  # create path to B and M  \n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=B and 1=M\n",
        "        \n",
        "        for i,img_path in enumerate(tqdm(os.listdir(path))):  # iterate over each image \n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img_path))  # convert to array\n",
        "                img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size                \n",
        "                img_paths.append(os.path.join(path,img_path))\n",
        "                \n",
        "                data.append([img_array, class_num])  # add this to our training_data\n",
        "                images.append(img_array)\n",
        "                labels.append(class_num)\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "\n",
        "    images = np.array(images)\n",
        "    images = np.asarray(images).astype(np.float32)\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "\n",
        "    return images,labels,img_paths \n",
        "    \n",
        "images,labels,img_paths = read_data()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3dfb039d75ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3dfb039d75ce>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get the classification  (0 or a 1). 0=B and 1=M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# iterate over each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# convert to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset1/ocrdata/0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF1ukVEgpJdX"
      },
      "source": [
        "'''test_images=[]\n",
        "train_images=[]\n",
        "test_labels=[]\n",
        "train_labels=[]\n",
        "index = []\n",
        "j=0\n",
        "for i in range (120):\n",
        " \n",
        " \n",
        "  if (j==10)|(j==11):\n",
        "    \n",
        "    test_images.append(images[i])\n",
        "    test_labels.append(labels[i])\n",
        "    index.append(i)\n",
        "  else:\n",
        "    \n",
        "    train_images.append(images[i])\n",
        "    train_labels.append(labels[i])\n",
        "\n",
        "  if j==11:\n",
        "    j=0\n",
        "  else:\n",
        "    j=j+1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUDLRBgS4hU-"
      },
      "source": [
        "\"\"\"\n",
        "print (index)\n",
        "train_labels = np.array(train_labels)\n",
        "train_images = np.array(train_images)\n",
        "test_labels = np.array(test_labels)\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "print(type(test_images))\n",
        "print(type(test_labels))\n",
        "print(type(train_images))\n",
        "print(type(train_labels))\n",
        "print (np.size(train_images))\n",
        "print (np.size(train_labels))\n",
        "print (np.size(test_images))\n",
        "print (np.size(test_labels))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq6FGIKsedJw"
      },
      "source": [
        "\"\"\"\n",
        "test_images = test_images/255\n",
        "train_images = train_images/255\n",
        "X = images\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbiE3X1UkPDI"
      },
      "source": [
        "class_names = [\"0\",\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "images =images/255\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(10):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(images[i*12])\n",
        "    plt.xlabel(class_names[labels[i*12]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldr3BOFGnr3O"
      },
      "source": [
        "model = models.Sequential()\n",
        "#i used add data augmentation instead of augmenting dataset we do it in runtime....or add in sequential(........)\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmA6Oa82usz_"
      },
      "source": [
        "def tf_cross_val_score(model, X, y, n_classes):\n",
        "    \"\"\"\n",
        "    Takes features, number of classes, labels and feature vector names and returns validation scores.\n",
        "    \n",
        "    X Numpy ndarray - features\n",
        "    y Numpy ndarray - labels\n",
        "    n_classes int - number of classes\n",
        "    \"\"\"\n",
        "    histories = []\n",
        "    scores = []\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    fold_no = 10\n",
        "    kf = KFold(n_splits = 2, random_state = None, shuffle = False)\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        #print(len(train_index))\n",
        "        X_train,y_train = np.array(X[train_index]),np.array(y[train_index])\n",
        "        X_test,y_test = np.array(X[test_index]),np.array(y[test_index])\n",
        "        y_train = tf.one_hot(y_train, NUM_CLASSES)\n",
        "        #print(len(y_train))\n",
        "        y_test = tf.one_hot(y_test, NUM_CLASSES)\n",
        "        \n",
        "        histres = model.fit(X_train,y_train, epochs=epochs, verbose=2)\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "        histories.append(histres)\n",
        "        score = model.evaluate(X_test,y_test, verbose=0)\n",
        "        scores.append(score)\n",
        "\n",
        "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]};')\n",
        "        fold_no += 1\n",
        "    return histories, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfMb7wVmn1Q7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EqXQnyfoHCA"
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBH_rStyoQqE"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXGqeYpFoaYe"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\"\"\"\n",
        "history = model.fit(train_images, train_labels, epochs= 20, batch_size=10,\n",
        "                    validation_data=(test_images, test_labels), verbose=2)\n",
        "\"\"\"\n",
        "histories,scores = tf_cross_val_score(model, X=images, y=labels, epochs= 20, n_classes=NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBDQ6fas3_NZ"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'Test accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXCEHBBncaCF"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "for i in range (0,10):\n",
        "  img_path = f'/content/drive/MyDrive/dataset1/predict/{i}.PNG'\n",
        "#  print('/content/drive/MyDrive/dataset1/predict/'{i}'.PNG')\n",
        "\n",
        "  img = keras.preprocessing.image.load_img(\n",
        "    img_path, target_size=(IMG_SIZE, IMG_SIZE)\n",
        "  )\n",
        "  img_array = keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "  predictions = model.predict(img_array)\n",
        "  score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "  print(\n",
        "    \"This image {} most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(i, class_names[np.argmax(score)], 100 * np.max(score))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jyRZGYebdFZ"
      },
      "source": [
        "!mkdir -p saved_model\n",
        "model.save('saved_model/urduocr')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}